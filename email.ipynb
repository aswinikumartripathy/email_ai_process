{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fb17524",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9193d8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "from llama_index.core import DocumentSummaryIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import PromptHelper\n",
    "from llama_index.core import ServiceContext\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core import GPTVectorStoreIndex\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.response_synthesizers import TreeSummarize\n",
    "from llama_index.core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f950294",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f63750ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4ee0e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45d65c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load environment variables\n",
    "def load_api_key():\n",
    "    load_dotenv()\n",
    "    openai.api_key= os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69c54406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Set global LlamaIndex settings\n",
    "def configure_llama_index():\n",
    "    Settings.llm = OpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n",
    "    Settings.embed_model = OpenAIEmbedding()\n",
    "    Settings.chunk_size_limit = 128\n",
    "    Settings.chunk_overlap = 50\n",
    "    Settings.num_output = 2048\n",
    "    prompt_helper = PromptHelper(\n",
    "        context_window = 4096,\n",
    "        num_output = 3000,\n",
    "        chunk_overlap_ratio = 0.1,\n",
    "        chunk_size_limit = 512,\n",
    "    )\n",
    "    Settings.prompt_helper = prompt_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee8ac770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create index using GPTVectorStoreIndex (which supports chunk retrieval)\n",
    "def build_vector_index(email_text: str) -> GPTVectorStoreIndex:\n",
    "    document  = Document(text=email_text)\n",
    "    index = GPTVectorStoreIndex.from_documents([document])\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f22ec65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorize emails into different categories\n",
    "def categorize_email(email_text: str) -> str:\n",
    "    # Build the vector index (make sure this function is defined elsewhere)\n",
    "    index = build_vector_index(email_text)\n",
    "\n",
    "    categories = [\"payment\", \"billing\", \"enrollment\", \"uncategorized\"]\n",
    "    category_query = f\"\"\"\n",
    "    Based on the relevant parts of this email, categorize it into one of the following categories:\n",
    "    {', '.join(categories)}. If it does not fit any of these categories, please categorize it as 'uncategorized'.\n",
    "    Respond only with the category name.\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve relevant chunks from the index\n",
    "    retriever = VectorIndexRetriever(index=index, similarity_top_k=3)\n",
    "    nodes = retriever.retrieve(category_query)\n",
    "\n",
    "    # Get total number of chunks\n",
    "    total_chunks = len(index.docstore.docs)\n",
    "    # print(f\"Total chunks in the index: {total_chunks}\")\n",
    "    # print(f\"Top {len(nodes)} relevant chunks retrieved:\\n\")\n",
    "\n",
    "    # for i, node in enumerate(nodes, 1):\n",
    "    #     print(f\"Chunk {i}:\\n{'-'*40}\\n{node.text.strip()}\\n\")\n",
    "\n",
    "    # Combine retrieved chunks\n",
    "    combined_text = \"\\n\".join([node.text for node in nodes])\n",
    "\n",
    "    # prompts for LLM\n",
    "    full_prompt = f\"\"\"\n",
    "    {combined_text}\n",
    "\n",
    "    Categorize this email based on the above content into one of: {', '.join(categories)}.\n",
    "\n",
    "    ---\n",
    "    1. Enrollment  \n",
    "    Categorize the email as Enrollment if any of the below keywords are found:  \n",
    "    a) AWD  \n",
    "    b) Enrollment  \n",
    "    c) Autopay  \n",
    "\n",
    "    2. Payment  \n",
    "    Categorize the email as Payment if any of the below keywords are found:  \n",
    "    a) Payment  \n",
    "    b) Cheque  \n",
    "    c) Remittance  \n",
    "    d) Invoice  \n",
    "    e) Coupon  \n",
    "    f) Credit  \n",
    "    g) Refund  \n",
    "\n",
    "    3. Billing  \n",
    "    Categorize the email as Billing if any of the below keywords are found:  \n",
    "    a) Billing  \n",
    "    b) Premium  \n",
    "    c) Invoice  \n",
    "    d) Incorrect Invoice  \n",
    "\n",
    "    Post-Processing Steps:\n",
    "\n",
    "    1. Autonomic Analysis Protocol  \n",
    "    - Automatically identify patterns and context beyond just keywords (e.g., \"I was charged wrongly\" implies billing).  \n",
    "    - Detect sentence structure and tone to infer category when explicit keywords are missing.\n",
    "\n",
    "    2. Primary Intent Detection  \n",
    "    - If multiple categories are detected, determine which intent is dominant based on keyword frequency, placement, and context.  \n",
    "    - Prioritize the category mentioned in the subject or first few lines.\n",
    "\n",
    "    3. Contradicting Evidence Check  \n",
    "    - Look for conflicting phrases (e.g., “Refund not received” implies Payment, not Billing).  \n",
    "    - Remove false positives caused by ambiguous keyword overlap (e.g., “Autopay invoice” likely relates to Enrollment, not Billing).\n",
    "\n",
    "    4. Priority Rules  \n",
    "    - If both Enrollment and Payment are detected, prioritize Enrollment.  \n",
    "    - If both Billing and Payment are detected, prioritize Payment.  \n",
    "    - If all three are mentioned, prioritize based on order: Enrollment > Payment > Billing.\n",
    "\n",
    "    5. Confidence Assessment  \n",
    "    - Assign a confidence score (0–100%) based on keyword density and clarity.  \n",
    "    - If confidence is below 60%, flag the result for manual review.\n",
    "\n",
    "    6. Integration of Atomic Signal  \n",
    "    - Include other metadata if available (e.g., subject line, tags, sender type) to refine prediction.  \n",
    "    - Example: If the sender is a known billing department, weight Billing higher.\n",
    "\n",
    "    7. Output Validation  \n",
    "    - Ensure the final category logically matches the context.  \n",
    "    - If mismatch found, re-apply rules from step 1 to 6.  \n",
    "    - Log output decision along with justification for traceability.\n",
    "\n",
    "    If none of the keywords match, categorize the email as 'uncategorized'.\n",
    "    Respond with only 1 word from the following : **payment**, **billing**, **enrollment**, **uncategorized**.\n",
    "    \"\"\"\n",
    "\n",
    "    response = Settings.llm.complete(full_prompt)\n",
    "    return response.text.strip().lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43b0589b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_existing_chunks(index: GPTVectorStoreIndex):\n",
    "    # return list of node object already used in the index\n",
    "    return list(index.docstore.docs.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa45cb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_summerize(index: GPTVectorStoreIndex, summary_window_words: int = 100) -> str:\n",
    "    nodes = get_existing_chunks(index)\n",
    "\n",
    "    summary_template = PromptTemplate(\n",
    "        f\"Summerize the following text into approximately {summary_window_words} words:\\n\\n\"\n",
    "        \"{{context_str}}\\n\\nSummary:\"\n",
    "    )\n",
    "\n",
    "    tree_summerizer = TreeSummarize(summary_template=summary_template)\n",
    "    summary_index = DocumentSummaryIndex(nodes)\n",
    "\n",
    "    query_engine = summary_index.as_query_engine(response_synthesizer=tree_summerizer)\n",
    "    response = query_engine.query(\"Please summerize the entire content of this email.\")\n",
    "\n",
    "    return response.response.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "354c77f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_score(original_text, generated_summary):\n",
    "    \"\"\"\n",
    "    Evaluate summary using cosine similarity between the original text and generated summary.\n",
    "    \n",
    "    Parameters:\n",
    "        original_text (str): The original text.\n",
    "        generated_summary (str): The generated summary.\n",
    "    \n",
    "    Returns:\n",
    "        float: Cosine similarity score.\n",
    "    \"\"\"\n",
    "    # Initialize the vectorizer\n",
    "    vectorizer = TfidfVectorizer().fit_transform([original_text, generated_summary])\n",
    "    # Compute cosine similarity between the original and generated text\n",
    "    cosine_sim = cosine_similarity(vectorizer[0:1], vectorizer[1:2])\n",
    "    \n",
    "    return cosine_sim[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2ec964c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_score_evaluation(original_text, generated_summary):\n",
    "    \"\"\"\n",
    "    Evaluate the generated summary using BERTScore, which compares embeddings.\n",
    "    \n",
    "    Parameters:\n",
    "        original_text (str): The original text.\n",
    "        generated_summary (str): The generated summary.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Precision, Recall, and F1 score from BERTScore.\n",
    "    \"\"\"\n",
    "    # Compute BERTScore\n",
    "    P, R, F1 = bert_score.score([generated_summary], [original_text], lang=\"en\")\n",
    "    \n",
    "    return {\"precision\": P.item(), \"recall\": R.item(), \"f1\": F1.item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08956632",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_email_from_file(file_path: str) -> str:\n",
    "    with open(file_path, 'r', encoding=\"utf-8\") as file:\n",
    "        email_text = file.read()\n",
    "    return email_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a76e703f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Category: payment\n",
      "current doc id: a4362b2c-9e10-4f57-90f0-3af1ad8605d9\n",
      "\n",
      "Email Summary: John Doe is inquiring about a delayed payment of $123.45 made via cheque on March 15. He requests clarification on the delay, potential refund processing, and any late payment charges. Additionally, he seeks options for setting up autopay to prevent future issues and asks for prompt assistance in resolving the matter.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    load_api_key()\n",
    "    configure_llama_index()\n",
    "\n",
    "    # Load email text from a file\n",
    "    email_text = load_email_from_file(\"emails/email1.txt\")\n",
    "    index = build_vector_index(email_text)\n",
    "\n",
    "    category = categorize_email(email_text)\n",
    "    print(f\"Predicted Category: {category}\")\n",
    "\n",
    "    summary = recursive_summerize(index, summary_window_words=50)\n",
    "    print(f\"\\nEmail Summary: {summary}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58a16dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.40011496420527337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore - Precision: 0.9122200608253479, Recall: 0.8355664014816284, F1: 0.872212290763855\n"
     ]
    }
   ],
   "source": [
    "similarity_score = cosine_similarity_score(email_text, summary)\n",
    "print(f\"Cosine Similarity: {similarity_score}\")\n",
    "bert_scores = bert_score_evaluation(email_text, summary)\n",
    "print(f\"BERTScore - Precision: {bert_scores['precision']}, Recall: {bert_scores['recall']}, F1: {bert_scores['f1']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1b458d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_email_for_categories(email_text, category_keywords_dict, model_name=\"all-MiniLM-L6-v2\"):\n",
    "    \"\"\"\n",
    "    Evaluate how well a single email belongs to multiple categories using category-specific keywords\n",
    "    by computing cosine similarity with the keywords of each category.\n",
    "    \n",
    "    Parameters:\n",
    "        email_text (str): The email text to be categorized.\n",
    "        category_keywords_dict (dict): A dictionary where keys are category names and values are lists of category keywords.\n",
    "        model_name (str): Pretrained sentence transformer model name.\n",
    "    \n",
    "    Returns:\n",
    "        str: The predicted category with the highest similarity.\n",
    "        float: The highest cosine similarity score.\n",
    "    \"\"\"\n",
    "    # Load sentence transformer model\n",
    "    model = SentenceTransformer(model_name)\n",
    "    \n",
    "    # Encode the email text\n",
    "    email_embedding = model.encode(email_text, convert_to_tensor=True)\n",
    "    \n",
    "    max_similarity = -1  # Initialize with a very low similarity score\n",
    "    predicted_category = \"\"\n",
    "    \n",
    "    # Iterate through each category and its keywords\n",
    "    for category, keywords in category_keywords_dict.items():\n",
    "        # Encode the category keywords\n",
    "        keyword_embeddings = model.encode(keywords, convert_to_tensor=True)\n",
    "        \n",
    "        # Calculate cosine similarity between the email and category keywords\n",
    "        sim_matrix = util.pytorch_cos_sim(email_embedding, keyword_embeddings)\n",
    "        \n",
    "        # Get the maximum similarity (best match for this category)\n",
    "        category_similarity = sim_matrix.max().item()\n",
    "        \n",
    "        # If the similarity score for this category is higher than the previous best, update the prediction\n",
    "        if category_similarity > max_similarity:\n",
    "            max_similarity = category_similarity\n",
    "            predicted_category = category\n",
    "    \n",
    "    return predicted_category, max_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97f6bf38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Category: Payment\n",
      "Cosine Similarity: 0.4528834819793701\n"
     ]
    }
   ],
   "source": [
    "# Define keywords for each category\n",
    "category_keywords_dict = {\n",
    "    \"Enrollment\": [\"AWD\", \"Enrollment\", \"Autopay\"],\n",
    "    \"Payment\": [\"Payment\", \"Cheque\", \"Remittance\", \"Invoice\", \"Coupon\", \"Credit\", \"Refund\"],\n",
    "    \"Billing\": [\"Billing\", \"Premium\", \"Invoice\", \"Incorrect Invoice\"]\n",
    "}\n",
    "\n",
    "# Evaluate how well the email fits with any of the categories\n",
    "predicted_category, similarity_score = evaluate_email_for_categories(email_text, category_keywords_dict)\n",
    "\n",
    "print(f\"Predicted Category: {predicted_category}\")\n",
    "print(f\"Cosine Similarity: {similarity_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e8f5ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myprojects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
